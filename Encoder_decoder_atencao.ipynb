{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N44EXiZIwpg_"
      },
      "outputs": [],
      "source": [
        "# Importando as bibliotecas necessárias.\n",
        "\n",
        "import os\n",
        "import pathlib\n",
        "import random\n",
        "import string\n",
        "import re\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow.data as tf_data\n",
        "import tensorflow.strings as tf_strings\n",
        "import tensorflow_datasets.public_api as tfds\n",
        "\n",
        "import keras\n",
        "from keras import layers\n",
        "from keras import ops\n",
        "from keras.layers import TextVectorization\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define o backend do Keras para o TensorFlow.\n",
        "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "lIEqT_Qew8A-",
        "outputId": "8e518023-e050-4f65-8ff0-c19d42aa8ec5"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'data.tsv'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-e55172c388e5>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Abre o arquivo \"data.tsv\" no modo de leitura com codificação UTF-8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data.tsv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;31m# Itera por cada linha do arquivo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data.tsv'"
          ]
        }
      ],
      "source": [
        "# Inicializa uma lista para armazenar os pares de textos\n",
        "text_pairs = []\n",
        "\n",
        "# Abre o arquivo \"data.tsv\" no modo de leitura com codificação UTF-8\n",
        "with open(\"data.tsv\", \"r\", encoding=\"utf-8\") as f:\n",
        "    # Itera por cada linha do arquivo\n",
        "    for line in f:\n",
        "        # Remove espaços extras e divide a linha utilizando a tabulação como separador\n",
        "        fields = line.strip().split(\"\\t\")\n",
        "        # Garante que a linha possui ao menos 4 campos (índices 0 a 3)\n",
        "        if len(fields) < 4:\n",
        "            continue\n",
        "        french = fields[1]  # Obtém a segunda coluna (texto em francês)\n",
        "        portuguese = \"[start] \" + fields[3] + \" [end]\"  # Obtém a quarta coluna (texto em português) e adiciona os marcadores de início e fim\n",
        "        text_pairs.append((french, portuguese))\n",
        "\n",
        "# Imprime o primeiro par de textos para verificação\n",
        "print(text_pairs[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BQZbopkCxjfB",
        "outputId": "d88b5a40-ffb9-4f73-978d-ec3625fc96aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "('Mais on peut aussi bien le concevoir comme un ensemble accolé.', '[start] Mas também é possível compreendê-lo como um todo. [end]')\n",
            "(\"Je comprends qu'une erreur a été commise.\", '[start] Eu entendo que um erro foi cometido. [end]')\n",
            "('Comment épelle-t-on «\\xa0pretty\\xa0»\\xa0?', '[start] Como se escreve \"pretty\"? [end]')\n",
            "(\"Tôt ou tard, j'entrerai en possession de l'héritage de mes parents.\", '[start] Cedo ou tarde, tomarei posse da herança dos meus pais. [end]')\n",
            "('Nous étions heureux.', '[start] Éramos felizes. [end]')\n"
          ]
        }
      ],
      "source": [
        "# Itera 5 vezes para imprimir 5 pares de textos escolhidos aleatoriamente\n",
        "for _ in range(5):\n",
        "    # Seleciona um par de textos aleatório da lista 'text_pairs' e o imprime\n",
        "    print(random.choice(text_pairs))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "coukIC5OyL1f",
        "outputId": "e1843b6f-2e81-499b-ce36-1743acf920b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "33030 total pairs\n",
            "23122 training pairs\n",
            "4954 validation pairs\n",
            "4954 test pairs\n"
          ]
        }
      ],
      "source": [
        "# Embaralha aleatoriamente os pares de textos\n",
        "random.shuffle(text_pairs)\n",
        "\n",
        "# Calcula o número de amostras para validação como 15% do total de pares\n",
        "num_val_samples = int(0.15 * len(text_pairs))\n",
        "\n",
        "# Define o número de amostras para treinamento, reservando 2 blocos de validação (um para validação e outro para teste)\n",
        "num_train_samples = len(text_pairs) - 2 * num_val_samples\n",
        "\n",
        "# Separa os pares para treinamento\n",
        "train_pairs = text_pairs[:num_train_samples]\n",
        "\n",
        "# Separa os pares para validação (logo após os pares de treinamento)\n",
        "val_pairs = text_pairs[num_train_samples : num_train_samples + num_val_samples]\n",
        "\n",
        "# Separa os pares para teste (os pares restantes após treinamento e validação)\n",
        "test_pairs = text_pairs[num_train_samples + num_val_samples :]\n",
        "\n",
        "# Exibe a quantidade total de pares, bem como a divisão em treinamento, validação e teste\n",
        "print(f\"{len(text_pairs)} total pairs\")\n",
        "print(f\"{len(train_pairs)} training pairs\")\n",
        "print(f\"{len(val_pairs)} validation pairs\")\n",
        "print(f\"{len(test_pairs)} test pairs\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_LmEwdu8yNv0"
      },
      "outputs": [],
      "source": [
        "# Define os caracteres de pontuação que serão removidos durante a padronização,\n",
        "# adicionando os caracteres \"«\" e \"»\".\n",
        "strip_chars = string.punctuation + \"«\" + \"»\"\n",
        "\n",
        "# Remove os colchetes dos caracteres a serem removidos, mantendo-os no texto\n",
        "strip_chars = strip_chars.replace(\"[\", \"\")\n",
        "strip_chars = strip_chars.replace(\"]\", \"\")\n",
        "\n",
        "# Define os parâmetros para o processamento dos textos\n",
        "vocab_size = 25000       # Tamanho máximo do vocabulário\n",
        "sequence_length = 20     # Comprimento máximo das sequências de saída\n",
        "batch_size = 64          # Tamanho do lote (batch)\n",
        "latentSpaceDimension = 32# Dimensão do espaço latente (latent space dimension).\n",
        "\n",
        "def custom_standardization(input_string):\n",
        "    \"\"\"\n",
        "    Função de padronização customizada para os textos.\n",
        "\n",
        "    Converte o texto para minúsculas e remove os caracteres definidos em 'strip_chars'.\n",
        "    \"\"\"\n",
        "    # Converte o texto para letras minúsculas\n",
        "    lowercase = tf_strings.lower(input_string)\n",
        "    # Remove os caracteres indesejados utilizando uma expressão regular\n",
        "    return tf_strings.regex_replace(lowercase, \"[%s]\" % re.escape(strip_chars), \"\")\n",
        "\n",
        "# Cria a camada de vetorização para os textos em francês\n",
        "french_vectorization = TextVectorization(\n",
        "    max_tokens=vocab_size,           # Número máximo de tokens\n",
        "    output_mode=\"int\",               # Saída como inteiros (índices dos tokens)\n",
        "    output_sequence_length=sequence_length,  # Comprimento fixo das sequências\n",
        ")\n",
        "\n",
        "# Cria a camada de vetorização para os textos em português,\n",
        "# utilizando a função de padronização customizada\n",
        "portuguese_vectorization = TextVectorization(\n",
        "    max_tokens=vocab_size,                # Número máximo de tokens\n",
        "    output_mode=\"int\",                    # Saída como inteiros (índices dos tokens)\n",
        "    output_sequence_length=sequence_length + 1,  # Comprimento fixo das sequências (+1 para token especial)\n",
        "    standardize=custom_standardization,   # Função customizada para padronização dos textos\n",
        ")\n",
        "\n",
        "# Extrai os textos em francês e português dos pares de treinamento\n",
        "train_french_texts = [pair[0] for pair in train_pairs]      # Lista de textos em francês\n",
        "train_portuguese_texts = [pair[1] for pair in train_pairs]    # Lista de textos em português\n",
        "\n",
        "# Adapta as camadas de vetorização aos textos de treinamento\n",
        "french_vectorization.adapt(train_french_texts)\n",
        "portuguese_vectorization.adapt(train_portuguese_texts)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BDCoAkUHyVLf"
      },
      "outputs": [],
      "source": [
        "def format_dataset(french, portuguese):\n",
        "    # Aplica a vetorização nos textos em francês\n",
        "    french = french_vectorization(french)\n",
        "    # Aplica a vetorização nos textos em português\n",
        "    portuguese = portuguese_vectorization(portuguese)\n",
        "    # Retorna um dicionário com as entradas para o codificador e decodificador,\n",
        "    # além dos rótulos para o treinamento do decodificador.\n",
        "    return (\n",
        "        {\n",
        "            \"encoder_inputs\": french,          # Entrada para o codificador\n",
        "            \"decoder_inputs\": portuguese[:, :-1],  # Entrada para o decodificador (exclui o último token)\n",
        "        },\n",
        "        portuguese[:, 1:],  # Rótulos para o decodificador (exclui o primeiro token)\n",
        "    )\n",
        "\n",
        "\n",
        "def make_dataset(pairs):\n",
        "    # Separa os textos em francês e português dos pares fornecidos\n",
        "    french_texts, portuguese_texts = zip(*pairs)\n",
        "    french_texts = list(french_texts)\n",
        "    portuguese_texts = list(portuguese_texts)\n",
        "\n",
        "    # Cria um dataset do TensorFlow a partir dos textos\n",
        "    dataset = tf_data.Dataset.from_tensor_slices((french_texts, portuguese_texts))\n",
        "    # Agrupa os exemplos em lotes (batch)\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    # Aplica a formatação dos dados utilizando a função format_dataset\n",
        "    dataset = dataset.map(format_dataset)\n",
        "    # Armazena o dataset em cache, embaralha e pré-carrega os lotes para otimizar o desempenho\n",
        "    return dataset.cache().shuffle(2048).prefetch(16)\n",
        "\n",
        "\n",
        "# Cria os datasets de treinamento e validação a partir dos pares correspondentes\n",
        "train_ds = make_dataset(train_pairs)\n",
        "val_ds = make_dataset(val_pairs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x3ChRmWUyX07",
        "outputId": "f6fe926c-8e34-4148-f833-bf28d26742ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "inputs[\"encoder_inputs\"].shape: (64, 20)\n",
            "inputs[\"decoder_inputs\"].shape: (64, 20)\n",
            "targets.shape: (64, 20)\n"
          ]
        }
      ],
      "source": [
        "# Itera sobre o primeiro lote do dataset de treinamento\n",
        "for inputs, targets in train_ds.take(1):\n",
        "    # Imprime a forma do tensor que contém as entradas para o codificador\n",
        "    print(f'inputs[\"encoder_inputs\"].shape: {inputs[\"encoder_inputs\"].shape}')\n",
        "\n",
        "    # Imprime a forma do tensor que contém as entradas para o decodificador\n",
        "    print(f'inputs[\"decoder_inputs\"].shape: {inputs[\"decoder_inputs\"].shape}')\n",
        "\n",
        "    # Imprime a forma do tensor que contém os alvos (saídas) para o treinamento do decodificador\n",
        "    print(f\"targets.shape: {targets.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Th_pfGu2yZf_",
        "outputId": "322ed110-0e61-487d-bf04-cc8d3b91b192"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Nous avons embarqué.', \"L'affinité et la communion de pensée entre ces deux âmes est comme si elles se connaissaient depuis d'autres vies.\", 'Je me rappelais les jours sereins et toujours les mêmes que, peu de semaines auparavant, je passais encore près de Marie, sans même entrevoir dans l’avenir une autre possibilité que celle d’un bonheur éternel.', \"Il est difficile de calculer les résultats de l'élection.\", 'Les dames sont jouées de différentes manières, malgré leur utilisation répandue dans tous les pays civilisés.']\n",
            "['[start] Nós embarcamos. [end]', '[start] A afinidade e a comunhão de pensamentos entre aquelas duas almas é tal, como se ambas se conhecessem desde outras vidas. [end]', '[start] Eu me lembrei daqueles dias calmos e sempre iguais, que, algumas semanas antes, ainda pudera passar ao lado de Maria, sem sequer vislumbrar no futuro senão a perspectiva de uma felicidade eterna. [end]', '[start] É difícil calcular os resultados da eleição. [end]', '[start] O jogo de damas é praticado de muitas maneiras diferentes, a despeito de sua grande difusão em todos os países civilizados. [end]']\n"
          ]
        }
      ],
      "source": [
        "# Imprime os 5 primeiros textos em francês dos dados de treinamento\n",
        "print(train_french_texts[:5])\n",
        "\n",
        "# Imprime os 5 primeiros textos em português dos dados de treinamento\n",
        "print(train_portuguese_texts[:5])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S9n-1i9pzTH_"
      },
      "outputs": [],
      "source": [
        "# Supondo que a classe de atenção Bahdanau já esteja definida corretamente:\n",
        "class BahdanauAttention(layers.Layer):\n",
        "    def __init__(self, units, verbose=0):\n",
        "        \"\"\"\n",
        "        Inicializa a camada de atenção Bahdanau.\n",
        "\n",
        "        Args:\n",
        "            units (int): Número de unidades para as camadas densas internas.\n",
        "            verbose (int, opcional): Nível de detalhamento para debug. Padrão é 0.\n",
        "        \"\"\"\n",
        "        super(BahdanauAttention, self).__init__()\n",
        "        # Camadas densas para transformar a query e os valores\n",
        "        self.W1 = layers.Dense(units)\n",
        "        self.W2 = layers.Dense(units)\n",
        "        # Camada densa para gerar os scores de atenção\n",
        "        self.V = layers.Dense(1)\n",
        "        self.verbose = verbose\n",
        "\n",
        "    def call(self, query, values):\n",
        "        \"\"\"\n",
        "        Executa a atenção Bahdanau.\n",
        "\n",
        "        Args:\n",
        "            query (Tensor): Vetor de consulta com shape (batch_size, hidden_size).\n",
        "            values (Tensor): Valores com shape (batch_size, time_steps, hidden_size).\n",
        "\n",
        "        Returns:\n",
        "            context_vector (Tensor): Vetor de contexto com shape (batch_size, hidden_size).\n",
        "            attention_weights (Tensor): Pesos de atenção com shape (batch_size, time_steps, 1).\n",
        "        \"\"\"\n",
        "        # Adiciona uma dimensão temporal à query para permitir a soma com os valores.\n",
        "        # Novo shape: (batch_size, 1, hidden_size)\n",
        "        query_with_time_axis = tf.expand_dims(query, 1)\n",
        "\n",
        "        # Calcula os scores de atenção combinando a query e os valores.\n",
        "        # A função tanh é aplicada após as transformações lineares.\n",
        "        # Resultado: (batch_size, time_steps, 1)\n",
        "        score = self.V(tf.nn.tanh(self.W1(query_with_time_axis) + self.W2(values)))\n",
        "\n",
        "        # Normaliza os scores usando softmax para obter os pesos de atenção.\n",
        "        # Shape: (batch_size, time_steps, 1)\n",
        "        attention_weights = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "        # Multiplica os pesos de atenção pelos valores para obter o vetor de contexto.\n",
        "        # Shape intermediário: (batch_size, time_steps, hidden_size)\n",
        "        context_vector = attention_weights * values\n",
        "\n",
        "        # Reduz a soma ao longo do eixo temporal para combinar as informações.\n",
        "        # Shape final: (batch_size, hidden_size)\n",
        "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "        return context_vector, attention_weights\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n7gZa9u_7J9Q"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Camada customizada para aplicar a atenção para cada timestep do decodificador\n",
        "class TimeDistributedAttention(layers.Layer):\n",
        "    def __init__(self, attention_layer, **kwargs):\n",
        "        \"\"\"\n",
        "        Inicializa a camada de atenção distribuída no tempo.\n",
        "\n",
        "        Args:\n",
        "            attention_layer (layers.Layer): Instância da camada de atenção a ser aplicada.\n",
        "            **kwargs: Argumentos adicionais para a camada pai.\n",
        "        \"\"\"\n",
        "        super(TimeDistributedAttention, self).__init__(**kwargs)\n",
        "        self.attention_layer = attention_layer\n",
        "\n",
        "    def call(self, inputs):\n",
        "        \"\"\"\n",
        "        Aplica a atenção a cada timestep dos outputs do decodificador.\n",
        "\n",
        "        Args:\n",
        "            inputs (list): Lista contendo [decoder_outputs, encoder_outputs] onde:\n",
        "                - decoder_outputs: (batch_size, dec_timesteps, latentSpaceDimension)\n",
        "                - encoder_outputs: (batch_size, enc_timesteps, latentSpaceDimension)\n",
        "\n",
        "        Returns:\n",
        "            context_sequence (Tensor): Sequência de vetores de contexto para cada timestep do decodificador,\n",
        "                                       com shape (batch_size, dec_timesteps, latentSpaceDimension).\n",
        "        \"\"\"\n",
        "        # Desempacota as entradas: outputs do decodificador e do codificador\n",
        "        decoder_outputs, encoder_outputs = inputs\n",
        "\n",
        "        # Transpõe os outputs do decodificador para iterar sobre o eixo temporal.\n",
        "        # Novo shape: (dec_timesteps, batch_size, latentSpaceDimension)\n",
        "        decoder_outputs_transposed = tf.transpose(decoder_outputs, perm=[1, 0, 2])\n",
        "\n",
        "        # Função que aplica a atenção para um único timestep do decodificador.\n",
        "        def apply_attention(decoder_timestep):\n",
        "            \"\"\"\n",
        "            Aplica a atenção para um timestep específico.\n",
        "\n",
        "            Args:\n",
        "                decoder_timestep (Tensor): Vetor do timestep do decodificador com shape (batch_size, latentSpaceDimension).\n",
        "\n",
        "            Returns:\n",
        "                context (Tensor): Vetor de contexto calculado para o timestep com shape (batch_size, latentSpaceDimension).\n",
        "            \"\"\"\n",
        "            # Utiliza o vetor do timestep como query para calcular a atenção sobre os encoder_outputs.\n",
        "            context, _ = self.attention_layer(decoder_timestep, encoder_outputs)\n",
        "            return context\n",
        "\n",
        "        # Aplica a função de atenção para cada timestep usando tf.map_fn.\n",
        "        # O resultado possui shape: (dec_timesteps, batch_size, latentSpaceDimension)\n",
        "        context_sequence_transposed = tf.map_fn(\n",
        "            apply_attention,\n",
        "            decoder_outputs_transposed,\n",
        "            fn_output_signature=tf.float32\n",
        "        )\n",
        "\n",
        "        # Transpõe de volta para o formato original: (batch_size, dec_timesteps, latentSpaceDimension)\n",
        "        context_sequence = tf.transpose(context_sequence_transposed, perm=[1, 0, 2])\n",
        "        return context_sequence\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uBJUAf9SyaOr"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, Model, Input\n",
        "\n",
        "class PositionalEmbedding(layers.Layer):\n",
        "    \"\"\"\n",
        "    Camada de embedding posicional que soma os embeddings dos tokens e das posições.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):\n",
        "        \"\"\"\n",
        "        Inicializa a camada de embedding posicional.\n",
        "\n",
        "        Args:\n",
        "            sequence_length (int): Comprimento máximo da sequência.\n",
        "            vocab_size (int): Tamanho do vocabulário.\n",
        "            embed_dim (int): Dimensão dos embeddings.\n",
        "            **kwargs: Argumentos adicionais para a classe base.\n",
        "        \"\"\"\n",
        "        super().__init__(**kwargs)\n",
        "        # Embedding para os tokens com dimensão de entrada igual ao tamanho do vocabulário\n",
        "        # e dimensão de saída igual a embed_dim\n",
        "        self.token_embeddings = layers.Embedding(\n",
        "            input_dim=vocab_size, output_dim=embed_dim\n",
        "        )\n",
        "        # Embedding para as posições com dimensão de entrada igual ao comprimento da sequência\n",
        "        # e dimensão de saída igual a embed_dim\n",
        "        self.position_embeddings = layers.Embedding(\n",
        "            input_dim=sequence_length, output_dim=embed_dim\n",
        "        )\n",
        "        self.sequence_length = sequence_length\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "    def call(self, inputs):\n",
        "        \"\"\"\n",
        "        Soma os embeddings dos tokens e das posições.\n",
        "\n",
        "        Args:\n",
        "            inputs (Tensor): Tensores contendo índices dos tokens com shape\n",
        "                             (batch_size, sequence_length).\n",
        "\n",
        "        Returns:\n",
        "            Tensor: Soma dos embeddings dos tokens e das posições com shape\n",
        "                    (batch_size, sequence_length, embed_dim).\n",
        "        \"\"\"\n",
        "        # Obtém os embeddings dos tokens: shape (batch_size, sequence_length, embed_dim)\n",
        "        embedded_tokens = self.token_embeddings(inputs)\n",
        "\n",
        "        # Cria um tensor constante com as posições: shape (sequence_length,)\n",
        "        positions = tf.range(start=0, limit=self.sequence_length, delta=1)\n",
        "\n",
        "        # Obtém os embeddings das posições: shape (sequence_length, embed_dim)\n",
        "        embedded_positions = self.position_embeddings(positions)\n",
        "\n",
        "        # Expande a dimensão para que os embeddings das posições possam ser somados\n",
        "        # aos embeddings dos tokens em todas as amostras do batch.\n",
        "        # Novo shape: (1, sequence_length, embed_dim)\n",
        "        embedded_positions = tf.expand_dims(embedded_positions, axis=0)\n",
        "\n",
        "        # Retorna a soma dos embeddings dos tokens com os embeddings das posições\n",
        "        return embedded_tokens + embedded_positions\n",
        "\n",
        "    def get_config(self):\n",
        "        \"\"\"\n",
        "        Retorna a configuração da camada para possibilitar a serialização.\n",
        "\n",
        "        Returns:\n",
        "            dict: Dicionário de configuração da camada.\n",
        "        \"\"\"\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"sequence_length\": self.sequence_length,\n",
        "            \"vocab_size\": self.vocab_size,\n",
        "            \"embed_dim\": self.embed_dim,\n",
        "        })\n",
        "        return config\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RHW-79jt5ZYr"
      },
      "outputs": [],
      "source": [
        "class AttentionLSTMCell(layers.Layer):\n",
        "    \"\"\"\n",
        "    Célula LSTM com mecanismo de atenção integrada.\n",
        "    Esta célula utiliza um mecanismo de atenção (attention_layer)\n",
        "    para calcular um vetor de contexto a cada timestep, que é concatenado\n",
        "    com o novo estado oculto da LSTM para formar a saída.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, units, attention_layer, **kwargs):\n",
        "        \"\"\"\n",
        "        Inicializa a célula LSTM com atenção.\n",
        "\n",
        "        Args:\n",
        "            units (int): Número de unidades da LSTM.\n",
        "            attention_layer (layers.Layer): Camada de atenção a ser utilizada.\n",
        "            **kwargs: Argumentos adicionais.\n",
        "        \"\"\"\n",
        "        super(AttentionLSTMCell, self).__init__(**kwargs)\n",
        "        self.units = units  # Número de unidades para a LSTM\n",
        "        self.attention_layer = attention_layer  # Mecanismo de atenção a ser aplicado\n",
        "        self.lstm_cell = layers.LSTMCell(units)  # Instância da célula LSTM padrão\n",
        "\n",
        "    @property\n",
        "    def state_size(self):\n",
        "        \"\"\"\n",
        "        Define o tamanho dos estados internos da célula.\n",
        "\n",
        "        Retorna uma lista contendo:\n",
        "          - hidden: vetor oculto com dimensão (units)\n",
        "          - cell: vetor de célula com dimensão (units)\n",
        "          - prev_attention: vetor de atenção do timestep anterior com dimensão (units)\n",
        "          - encoder_outputs: estados do encoder com shape (None, units)\n",
        "            (None indica que o número de timesteps do encoder pode variar)\n",
        "        \"\"\"\n",
        "        return [self.units, self.units, self.units, tf.TensorShape([None, self.units])]\n",
        "\n",
        "    @property\n",
        "    def output_size(self):\n",
        "        \"\"\"\n",
        "        Define o tamanho da saída da célula.\n",
        "\n",
        "        A saída é a concatenação do novo estado oculto e do vetor de contexto,\n",
        "        portanto, sua dimensão é a soma das dimensões dos dois vetores.\n",
        "        \"\"\"\n",
        "        return self.units + self.units\n",
        "\n",
        "    def call(self, inputs, states):\n",
        "        \"\"\"\n",
        "        Executa um passo de tempo da célula LSTM com atenção.\n",
        "\n",
        "        Args:\n",
        "            inputs (Tensor): Embedding do token atual com shape (batch_size, embed_dim).\n",
        "            states (list): Lista contendo os estados anteriores:\n",
        "                [hidden, cell, prev_attention, encoder_outputs]\n",
        "\n",
        "        Retorna:\n",
        "            new_output (Tensor): Saída da célula com shape (batch_size, output_size),\n",
        "                                 que é a concatenação do novo estado oculto e do vetor de contexto.\n",
        "            new_state (list): Lista atualizada dos estados, mantendo os valores para:\n",
        "                              [new_hidden, new_cell, context_vector, encoder_outputs].\n",
        "        \"\"\"\n",
        "        # Desempacota os estados: hidden, cell, atenção anterior e outputs do encoder\n",
        "        hidden, cell, prev_attention, encoder_outputs = states\n",
        "\n",
        "        # Concatena o embedding do token atual com o vetor de atenção do timestep anterior (input feeding)\n",
        "        input_combined = tf.concat([inputs, prev_attention], axis=-1)\n",
        "\n",
        "        # Processa o input combinado pela LSTMCell para obter o novo estado oculto e o novo estado da célula\n",
        "        output, [new_hidden, new_cell] = self.lstm_cell(input_combined, [hidden, cell])\n",
        "\n",
        "        # Calcula a atenção utilizando o novo estado oculto como query e os outputs do encoder como values\n",
        "        context_vector, attention_weights = self.attention_layer(new_hidden, encoder_outputs)\n",
        "\n",
        "        # Define a saída da célula como a concatenação do novo estado oculto com o vetor de contexto\n",
        "        new_output = tf.concat([new_hidden, context_vector], axis=-1)\n",
        "\n",
        "        # Atualiza os estados:\n",
        "        # - new_hidden: novo estado oculto da LSTM\n",
        "        # - new_cell: novo estado da célula da LSTM\n",
        "        # - context_vector: vetor de contexto obtido pela atenção (será usado no próximo timestep)\n",
        "        # - encoder_outputs: permanece constante ao longo da decodificação\n",
        "        new_state = [new_hidden, new_cell, context_vector, encoder_outputs]\n",
        "\n",
        "        return new_output, new_state\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zEmxKVc0TUst",
        "outputId": "1c7ff268-4749-49a7-f76b-aa27986138bf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:393: UserWarning: `build()` was called on layer 'attention_lstm_cell_8', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Camada densa para adaptar os outputs do encoder para o espaço latente\n",
        "encoder_adapter = layers.Dense(latentSpaceDimension, activation='relu', name='encoder_adapter')\n",
        "\n",
        "# --- Parâmetros e camadas de embedding (assumindo que já estejam definidos) ---\n",
        "# sequence_length: comprimento máximo da sequência\n",
        "# vocab_size: tamanho do vocabulário\n",
        "# embed_dim: dimensão dos embeddings dos tokens\n",
        "# latentSpaceDimension: dimensão do espaço latente\n",
        "\n",
        "# --- Encoder ---\n",
        "# Entrada para o encoder: sequência de índices (tokens) com tamanho variável\n",
        "encoder_inputs = Input(shape=(None,), dtype=\"int64\", name=\"encoder_inputs\")\n",
        "# Aplica o embedding posicional nos tokens do encoder\n",
        "encoder_embed = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs)\n",
        "# Camada LSTM do encoder que retorna tanto as sequências quanto os estados finais\n",
        "encoder_lstm = layers.LSTM(latentSpaceDimension,\n",
        "                           return_sequences=True,\n",
        "                           return_state=True,\n",
        "                           name='encoder_lstm')\n",
        "# Processa a sequência de embeddings e obtém os outputs e os estados finais (hidden e cell)\n",
        "encoder_outputs, state_h, state_c = encoder_lstm(encoder_embed)\n",
        "# Armazena os estados finais do encoder para uso posterior no decoder\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "# Adapta os outputs do encoder para o espaço latente usando a camada densa definida anteriormente\n",
        "adapted_encoder_outputs = encoder_adapter(encoder_outputs)\n",
        "\n",
        "# --- Decoder ---\n",
        "# Entrada para o decoder: sequência de índices (tokens) com tamanho variável\n",
        "decoder_inputs = Input(shape=(None,), dtype=\"int64\", name=\"decoder_inputs\")\n",
        "# Aplica a camada de embedding para os tokens do decoder\n",
        "decoder_embeddings = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)(decoder_inputs)\n",
        "\n",
        "# Cria o cell customizado de LSTM com atenção (input feeding) utilizando a AttentionLSTMCell\n",
        "attention_cell = AttentionLSTMCell(latentSpaceDimension, attention_layer)\n",
        "# Cria uma camada RNN que utiliza a célula customizada, retornando sequências e estados\n",
        "decoder_rnn = layers.RNN(attention_cell, return_sequences=True, return_state=True)\n",
        "\n",
        "# Função auxiliar para inicializar o vetor de atenção com zeros para cada exemplo do batch\n",
        "def get_initial_attention(x):\n",
        "    batch_size = tf.shape(x)[0]\n",
        "    return tf.zeros((batch_size, latentSpaceDimension), dtype=tf.float32)\n",
        "\n",
        "# Inicializa o vetor de atenção com zeros, utilizando o estado hidden do encoder como base para definir o batch size\n",
        "initial_attention = layers.Lambda(get_initial_attention)(state_h)\n",
        "\n",
        "# Define o estado inicial do decoder como a combinação dos estados do encoder e o vetor de atenção inicial:\n",
        "# [hidden, cell, prev_attention, adapted_encoder_outputs]\n",
        "initial_state = encoder_states + [initial_attention, adapted_encoder_outputs]\n",
        "\n",
        "# Executa a RNN do decoder, processando os embeddings do decoder com o estado inicial definido\n",
        "decoder_rnn_outputs = decoder_rnn(decoder_embeddings, initial_state=initial_state)\n",
        "# A saída do decoder é a sequência resultante (primeiro elemento do retorno da RNN)\n",
        "decoder_outputs = decoder_rnn_outputs[0]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        },
        "id": "H6kTExKzyeId",
        "outputId": "e2482b2c-1b7a-4e6e-ed12-943dd5cf17a5"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"seq2seq_input_feeding\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"seq2seq_input_feeding\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)              </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">        Param # </span>┃<span style=\"font-weight: bold\"> Connected to           </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ encoder_inputs            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)           │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ positional_embedding_14   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,922,560</span> │ encoder_inputs[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">PositionalEmbedding</span>)     │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ decoder_inputs            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)           │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ encoder_lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)       │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>),       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">20,608</span> │ positional_embedding_… │\n",
              "│                           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,     │                │                        │\n",
              "│                           │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)]                   │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ embedding_38 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,920,000</span> │ decoder_inputs[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ lambda_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ encoder_lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>]     │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ encoder_adapter (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">1,056</span> │ encoder_lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ rnn_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">RNN</span>)               │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>),     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">2,145</span> │ embedding_38[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],    │\n",
              "│                           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,     │                │ encoder_lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>],    │\n",
              "│                           │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)]       │                │ encoder_lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>],    │\n",
              "│                           │                        │                │ lambda_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n",
              "│                           │                        │                │ encoder_adapter[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ decoder_dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15000</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">975,000</span> │ rnn_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ encoder_inputs            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)           │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ positional_embedding_14   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │      \u001b[38;5;34m1,922,560\u001b[0m │ encoder_inputs[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "│ (\u001b[38;5;33mPositionalEmbedding\u001b[0m)     │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ decoder_inputs            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)           │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ encoder_lstm (\u001b[38;5;33mLSTM\u001b[0m)       │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m32\u001b[0m),       │         \u001b[38;5;34m20,608\u001b[0m │ positional_embedding_… │\n",
              "│                           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,     │                │                        │\n",
              "│                           │ \u001b[38;5;34m32\u001b[0m)]                   │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ embedding_38 (\u001b[38;5;33mEmbedding\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │      \u001b[38;5;34m1,920,000\u001b[0m │ decoder_inputs[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ lambda_8 (\u001b[38;5;33mLambda\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │              \u001b[38;5;34m0\u001b[0m │ encoder_lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m1\u001b[0m]     │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ encoder_adapter (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m32\u001b[0m)         │          \u001b[38;5;34m1,056\u001b[0m │ encoder_lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ rnn_8 (\u001b[38;5;33mRNN\u001b[0m)               │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m),     │          \u001b[38;5;34m2,145\u001b[0m │ embedding_38[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],    │\n",
              "│                           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,     │                │ encoder_lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m1\u001b[0m],    │\n",
              "│                           │ \u001b[38;5;34m32\u001b[0m), (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)]       │                │ encoder_lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m2\u001b[0m],    │\n",
              "│                           │                        │                │ lambda_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n",
              "│                           │                        │                │ encoder_adapter[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ decoder_dense (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15000\u001b[0m)    │        \u001b[38;5;34m975,000\u001b[0m │ rnn_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,841,369</span> (18.47 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,841,369\u001b[0m (18.47 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,841,369</span> (18.47 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m4,841,369\u001b[0m (18.47 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Camada densa final para gerar as predições a partir dos outputs do decoder.\n",
        "decoder_dense = layers.Dense(vocab_size, activation='softmax', name='decoder_dense')\n",
        "final_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "# --- Modelo Final ---\n",
        "# Define o modelo sequencial que recebe as entradas do encoder e do decoder,\n",
        "# e produz as predições finais.\n",
        "model = Model([encoder_inputs, decoder_inputs], final_outputs, name='seq2seq_input_feeding')\n",
        "\n",
        "# Compila o modelo utilizando o otimizador Adam, a função de perda\n",
        "# de entropia cruzada esparsa e a métrica de acurácia.\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Exibe um resumo da arquitetura do modelo.\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "jfo-sAwWyg03",
        "outputId": "f5dcccfd-2db2-477e-c7e7-cdf19a845fb1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m377s\u001b[0m 1s/step - accuracy: 0.6427 - loss: 4.8249 - val_accuracy: 0.7024 - val_loss: 2.1633\n",
            "Epoch 2/5\n",
            "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m382s\u001b[0m 1s/step - accuracy: 0.7024 - loss: 2.1212 - val_accuracy: 0.7146 - val_loss: 2.0174\n",
            "Epoch 3/5\n",
            "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m394s\u001b[0m 1s/step - accuracy: 0.7165 - loss: 1.9576 - val_accuracy: 0.7246 - val_loss: 1.9211\n",
            "Epoch 4/5\n",
            "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m429s\u001b[0m 1s/step - accuracy: 0.7297 - loss: 1.8318 - val_accuracy: 0.7366 - val_loss: 1.8310\n",
            "Epoch 5/5\n",
            "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m387s\u001b[0m 1s/step - accuracy: 0.7405 - loss: 1.7253 - val_accuracy: 0.7435 - val_loss: 1.7545\n",
            "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 225ms/step - accuracy: 0.7407 - loss: 1.7767\n",
            "Test loss: 1.7544641494750977\n",
            "Test accuracy: 0.7434595823287964\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# Preparação dos arrays para treinamento e teste a partir dos textos brutos\n",
        "# =============================================================================\n",
        "\n",
        "# --- Dados de Treinamento ---\n",
        "# Converte os textos de treinamento para arrays utilizando as camadas de vetorização.\n",
        "# O resultado é um array de inteiros para o encoder.\n",
        "encoder_input_data = french_vectorization(np.array(train_french_texts)).numpy()\n",
        "\n",
        "# Converte os textos de treinamento do português para arrays tokenizados.\n",
        "# O resultado é um array onde cada linha representa uma sequência tokenizada.\n",
        "portuguese_tokenized = portuguese_vectorization(np.array(train_portuguese_texts)).numpy()\n",
        "\n",
        "# Para o decoder, definimos:\n",
        "# - decoder_input_data: sequência de entrada do decoder (todos os tokens, exceto o último),\n",
        "#   pois o modelo recebe essa sequência para prever o próximo token.\n",
        "# - decoder_target_data: sequência alvo (todos os tokens, exceto o primeiro), que é o que o\n",
        "#   modelo deve prever.\n",
        "decoder_input_data = portuguese_tokenized[:, :-1]\n",
        "decoder_target_data = portuguese_tokenized[:, 1:]\n",
        "\n",
        "# --- Dados de Teste ---\n",
        "# Extrai os textos de teste dos pares e converte utilizando as camadas de vetorização.\n",
        "\n",
        "# Obtém os textos em francês e português dos pares de teste\n",
        "test_french_texts = [pair[0] for pair in test_pairs]\n",
        "test_portuguese_texts = [pair[1] for pair in test_pairs]\n",
        "\n",
        "# Converte os textos de teste para arrays de inteiros\n",
        "encoder_input_test = french_vectorization(np.array(test_french_texts)).numpy()\n",
        "portuguese_tokenized_test = portuguese_vectorization(np.array(test_portuguese_texts)).numpy()\n",
        "\n",
        "# Prepara as sequências para o decoder no teste, seguindo a mesma lógica aplicada aos dados de treinamento.\n",
        "decoder_input_test = portuguese_tokenized_test[:, :-1]\n",
        "decoder_target_test = portuguese_tokenized_test[:, 1:]\n",
        "\n",
        "# =============================================================================\n",
        "# Treinamento e Avaliação do Modelo com a Nova Arquitetura (com atenção)\n",
        "# =============================================================================\n",
        "\n",
        "# Treina o modelo utilizando os arrays preparados para o encoder e decoder.\n",
        "history = model.fit(\n",
        "    x=[encoder_input_data, decoder_input_data],\n",
        "    y=decoder_target_data,\n",
        "    batch_size=64,\n",
        "    epochs=5,\n",
        "    validation_data=([encoder_input_test, decoder_input_test], decoder_target_test)\n",
        ")\n",
        "\n",
        "# Avalia o modelo no conjunto de teste.\n",
        "test_loss, test_accuracy = model.evaluate(\n",
        "    x=[encoder_input_test, decoder_input_test],\n",
        "    y=decoder_target_test\n",
        ")\n",
        "\n",
        "# Exibe os resultados da avaliação no teste.\n",
        "print(\"Test loss:\", test_loss)\n",
        "print(\"Test accuracy:\", test_accuracy)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yfMUdro2yo9A"
      },
      "outputs": [],
      "source": [
        "# Resume o histórico de acurácia: plota a acurácia para os conjuntos de treinamento e validação\n",
        "plt.plot(history.history['accuracy'])       # Acurácia durante o treinamento\n",
        "plt.plot(history.history['val_accuracy'])     # Acurácia durante a validação\n",
        "plt.title(model.name + ' accuracy')           # Define o título do gráfico (nome do modelo + 'accuracy')\n",
        "plt.ylabel('accuracy')                        # Rótulo do eixo y\n",
        "plt.xlabel('epoch')                           # Rótulo do eixo x\n",
        "plt.legend(['train', 'val'], loc='upper left')  # Adiciona a legenda indicando os conjuntos\n",
        "plt.show()                                    # Exibe o gráfico de acurácia\n",
        "\n",
        "# Resume o histórico de perda: plota a perda para os conjuntos de treinamento e validação\n",
        "plt.plot(history.history['loss'])             # Perda durante o treinamento\n",
        "plt.plot(history.history['val_loss'])           # Perda durante a validação\n",
        "plt.title(model.name + ' loss')                # Define o título do gráfico (nome do modelo + 'loss')\n",
        "plt.ylabel('loss')                            # Rótulo do eixo y\n",
        "plt.xlabel('epoch')                           # Rótulo do eixo x\n",
        "plt.legend(['train', 'val'], loc='upper left')  # Adiciona a legenda indicando os conjuntos\n",
        "plt.show()                                    # Exibe o gráfico de perda\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
